{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model3_BERTcoref",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAxGbBQYXHy-",
        "colab_type": "text"
      },
      "source": [
        "# Эксперимент №2. SpanBERT+BERT-Chinese \n",
        "\n",
        "За основу берем скрипты [BERT for Coreference Resolution](https://github.com/mandarjoshi90/coref), которые обучаются на СoNLL2012, они были немного изменены, т.к. не работают с китайским языком.\n",
        "\n",
        "В ходе экспериментов были протестировано несколько предобученных BERT-моделей. Наилучший результат показала модель [Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm), поэтому используем её. \n",
        "\n",
        "Сама модель - это Берт + несколько слоёв. Тип модели: [e2e-coref model](https://github.com/kentonl/e2e-coref).\n",
        "\n",
        "Структура модели (последний слой Берта и слои для определения спанов)\n",
        "```\n",
        "  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
        "  name = span_width_embeddings:0, shape = (30, 20)\n",
        "  name = mention_word_attn/output_weights:0, shape = (768, 1)\n",
        "  name = mention_word_attn/output_bias:0, shape = (1,)\n",
        "  name = mention_scores/hidden_weights_0:0, shape = (2324, 3000)\n",
        "  name = mention_scores/hidden_bias_0:0, shape = (3000,)\n",
        "  name = mention_scores/output_weights:0, shape = (3000, 1)\n",
        "  name = mention_scores/output_bias:0, shape = (1,)\n",
        "  name = span_width_prior_embeddings:0, shape = (30, 20)\n",
        "  name = width_scores/hidden_weights_0:0, shape = (20, 3000)\n",
        "  name = width_scores/hidden_bias_0:0, shape = (3000,)\n",
        "  name = width_scores/output_weights:0, shape = (3000, 1)\n",
        "  name = width_scores/output_bias:0, shape = (1,)\n",
        "  name = genre_embeddings:0, shape = (7, 20)\n",
        "  name = src_projection/output_weights:0, shape = (2324, 2324)\n",
        "  name = src_projection/output_bias:0, shape = (2324,)\n",
        "  name = antecedent_distance_emb:0, shape = (10, 20)\n",
        "  name = output_weights:0, shape = (20, 1)\n",
        "  name = output_bias:0, shape = (1,)\n",
        "  name = coref_layer/same_speaker_emb:0, shape = (2, 20)\n",
        "  name = coref_layer/antecedent_distance_emb:0, shape = (10, 20)\n",
        "  name = coref_layer/segment_distance/segment_distance_embeddings:0, shape = (11, 20)\n",
        "  name = coref_layer/slow_antecedent_scores/hidden_weights_0:0, shape = (7052, 3000)\n",
        "  name = coref_layer/slow_antecedent_scores/hidden_bias_0:0, shape = (3000,)\n",
        "  name = coref_layer/slow_antecedent_scores/output_weights:0, shape = (3000, 1)\n",
        "  name = coref_layer/slow_antecedent_scores/output_bias:0, shape = (1,)\n",
        "  name = coref_layer/f/output_weights:0, shape = (4648, 2324)\n",
        "  name = coref_layer/f/output_bias:0, shape = (2324,)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ziBjEDrYOiV",
        "colab_type": "text"
      },
      "source": [
        "Папка для сохранения всех данных - генерации датасетов, сохранения логов, сохранения обученной модели."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoDsVF9jV0oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS87qpNKaM9p",
        "colab_type": "text"
      },
      "source": [
        "Копируем OntoNotes 5.0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_UMHxNxWoLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/drive/My Drive/colab_data/Coreference/span_bert/data/ontonotes-release-5.0_LDC2013T19.tgz' ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr2vmm9kWuTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar xvzf /content/ontonotes-release-5.0_LDC2013T19.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n94gB8qW-t6",
        "colab_type": "code",
        "outputId": "1730a3de-b0d8-4763-a182-a7cb8941f9fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/blanchefort/coref.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'coref'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects:  10% (1/10)\u001b[K\rremote: Counting objects:  20% (2/10)\u001b[K\rremote: Counting objects:  30% (3/10)\u001b[K\rremote: Counting objects:  40% (4/10)\u001b[K\rremote: Counting objects:  50% (5/10)\u001b[K\rremote: Counting objects:  60% (6/10)\u001b[K\rremote: Counting objects:  70% (7/10)\u001b[K\rremote: Counting objects:  80% (8/10)\u001b[K\rremote: Counting objects:  90% (9/10)\u001b[K\rremote: Counting objects: 100% (10/10)\u001b[K\rremote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 741 (delta 3), reused 6 (delta 2), pack-reused 731\n",
            "Receiving objects: 100% (741/741), 4.18 MiB | 21.08 MiB/s, done.\n",
            "Resolving deltas: 100% (444/444), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0vTyjJjadRp",
        "colab_type": "text"
      },
      "source": [
        "Переходим в папку скачанных скриптов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy9Oan9JXunb",
        "colab_type": "code",
        "outputId": "16209547-eaaa-48bf-87ca-d48e2d467bee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd coref"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/coref\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Th73KdXahUa",
        "colab_type": "text"
      },
      "source": [
        "Устанавливаем зависимости:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfulQVELXyTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat requirements.txt | sed 's/MarkupSafe==1.0/MarkupSafe==1.1.1/' > tmp\n",
        "!mv tmp requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htlftvtOBfdV",
        "colab_type": "code",
        "outputId": "c247d507-2a1b-4978-d24a-2b4680d95298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install -r requirements.txt --log install-log.txt -q"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkG58eN6DLRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./setup_all.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j53ufokWP69_",
        "colab_type": "text"
      },
      "source": [
        "Генерируем датасеты из OntoNotes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmuJtH3FX4dE",
        "colab_type": "code",
        "outputId": "7b587390-5646-418f-e93a-c5745d5fc080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! ./setup_training.sh '/content/ontonotes-release-5.0' '/content/data' > /content/setup_log.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-12 14:15:02--  http://conll.cemantix.org/2012/download/conll-2012-train.v4.tar.gz\n",
            "Resolving conll.cemantix.org (conll.cemantix.org)... 66.147.244.155\n",
            "Connecting to conll.cemantix.org (conll.cemantix.org)|66.147.244.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 71105451 (68M) [application/x-gzip]\n",
            "Saving to: ‘/content/data/conll-2012-train.v4.tar.gz’\n",
            "\n",
            "conll-2012-train.v4 100%[===================>]  67.81M  6.62MB/s    in 8.9s    \n",
            "\n",
            "2020-05-12 14:15:11 (7.60 MB/s) - ‘/content/data/conll-2012-train.v4.tar.gz’ saved [71105451/71105451]\n",
            "\n",
            "--2020-05-12 14:15:16--  http://conll.cemantix.org/2012/download/conll-2012-development.v4.tar.gz\n",
            "Resolving conll.cemantix.org (conll.cemantix.org)... 66.147.244.155\n",
            "Connecting to conll.cemantix.org (conll.cemantix.org)|66.147.244.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9270299 (8.8M) [application/x-gzip]\n",
            "Saving to: ‘/content/data/conll-2012-development.v4.tar.gz’\n",
            "\n",
            "conll-2012-developm 100%[===================>]   8.84M  7.01MB/s    in 1.3s    \n",
            "\n",
            "2020-05-12 14:15:17 (7.01 MB/s) - ‘/content/data/conll-2012-development.v4.tar.gz’ saved [9270299/9270299]\n",
            "\n",
            "--2020-05-12 14:15:18--  http://conll.cemantix.org/2012/download/test/conll-2012-test-key.tar.gz\n",
            "Resolving conll.cemantix.org (conll.cemantix.org)... 66.147.244.155\n",
            "Connecting to conll.cemantix.org (conll.cemantix.org)|66.147.244.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4784017 (4.6M) [application/x-gzip]\n",
            "Saving to: ‘/content/data/conll-2012-test-key.tar.gz’\n",
            "\n",
            "conll-2012-test-key 100%[===================>]   4.56M  6.76MB/s    in 0.7s    \n",
            "\n",
            "2020-05-12 14:15:18 (6.76 MB/s) - ‘/content/data/conll-2012-test-key.tar.gz’ saved [4784017/4784017]\n",
            "\n",
            "--2020-05-12 14:15:19--  http://conll.cemantix.org/2012/download/test/conll-2012-test-official.v9.tar.gz\n",
            "Resolving conll.cemantix.org (conll.cemantix.org)... 66.147.244.155\n",
            "Connecting to conll.cemantix.org (conll.cemantix.org)|66.147.244.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4088019 (3.9M) [application/x-gzip]\n",
            "Saving to: ‘/content/data/conll-2012-test-official.v9.tar.gz’\n",
            "\n",
            "conll-2012-test-off 100%[===================>]   3.90M  8.86MB/s    in 0.4s    \n",
            "\n",
            "2020-05-12 14:15:19 (8.86 MB/s) - ‘/content/data/conll-2012-test-official.v9.tar.gz’ saved [4088019/4088019]\n",
            "\n",
            "--2020-05-12 14:15:19--  http://conll.cemantix.org/2012/download/conll-2012-scripts.v3.tar.gz\n",
            "Resolving conll.cemantix.org (conll.cemantix.org)... 66.147.244.155\n",
            "Connecting to conll.cemantix.org (conll.cemantix.org)|66.147.244.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15839 (15K) [application/x-gzip]\n",
            "Saving to: ‘/content/data/conll-2012-scripts.v3.tar.gz’\n",
            "\n",
            "conll-2012-scripts. 100%[===================>]  15.47K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-05-12 14:15:20 (369 KB/s) - ‘/content/data/conll-2012-scripts.v3.tar.gz’ saved [15839/15839]\n",
            "\n",
            "--2020-05-12 14:15:20--  http://conll.cemantix.org/download/reference-coreference-scorers.v8.01.tar.gz\n",
            "Resolving conll.cemantix.org (conll.cemantix.org)... 66.147.244.155\n",
            "Connecting to conll.cemantix.org (conll.cemantix.org)|66.147.244.155|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52248 (51K) [application/x-gzip]\n",
            "Saving to: ‘/content/data/reference-coreference-scorers.v8.01.tar.gz’\n",
            "\n",
            "reference-coreferen 100%[===================>]  51.02K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-05-12 14:15:20 (831 KB/s) - ‘/content/data/reference-coreference-scorers.v8.01.tar.gz’ saved [52248/52248]\n",
            "\n",
            "--2020-05-12 14:15:20--  https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 2607:f8b0:400e:c09::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 381892918 (364M) [application/zip]\n",
            "Saving to: ‘/content/data/chinese_L-12_H-768_A-12.zip’\n",
            "\n",
            "chinese_L-12_H-768_ 100%[===================>] 364.20M   100MB/s    in 3.6s    \n",
            "\n",
            "2020-05-12 14:15:24 (100 MB/s) - ‘/content/data/chinese_L-12_H-768_A-12.zip’ saved [381892918/381892918]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMOO3qJHbLWK",
        "colab_type": "text"
      },
      "source": [
        "Генерируем файлы для обучения (получаются JSON-файлы). Важно указать правильный словарь. Нужно использовать именно словарь выбранной модели, т.к. другие словари дают плохое качество для китайского языка. Словарь всегда идёт вместе с предобученной моделью."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FeNWJ7LWvdr",
        "colab_type": "code",
        "outputId": "9da76199-b447-4c44-84d6-361d3372e2c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "!python ./minimize.py /content/data/chinese_L-12_H-768_A-12/vocab.txt /content/data /content/data true"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0512 14:48:56.707460 140240500500352 deprecation_wrapper.py:119] From /content/coref/coref_ops.py:11: The name tf.NotDifferentiable is deprecated. Please use tf.no_gradient instead.\n",
            "\n",
            "W0512 14:48:56.734206 140240500500352 deprecation_wrapper.py:119] From /content/coref/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0512 14:48:58.872298 140240500500352 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "True\n",
            "W0512 14:48:58.872742 140240500500352 deprecation_wrapper.py:119] From /content/coref/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Minimizing /content/data/dev.chinese.v4_gold_conll\n",
            "Wrote 252 documents to /content/data/dev.chinese.128.jsonlines\n",
            "Minimizing /content/data/train.chinese.v4_gold_conll\n",
            "Merging clusters (shouldn't happen very often.)\n",
            "Wrote 1810 documents to /content/data/train.chinese.128.jsonlines\n",
            "Minimizing /content/data/test.chinese.v4_gold_conll\n",
            "Wrote 218 documents to /content/data/test.chinese.128.jsonlines\n",
            "Minimizing /content/data/dev.chinese.v4_gold_conll\n",
            "Wrote 252 documents to /content/data/dev.chinese.256.jsonlines\n",
            "Minimizing /content/data/train.chinese.v4_gold_conll\n",
            "Merging clusters (shouldn't happen very often.)\n",
            "Wrote 1810 documents to /content/data/train.chinese.256.jsonlines\n",
            "Minimizing /content/data/test.chinese.v4_gold_conll\n",
            "Wrote 218 documents to /content/data/test.chinese.256.jsonlines\n",
            "Minimizing /content/data/dev.chinese.v4_gold_conll\n",
            "Wrote 252 documents to /content/data/dev.chinese.384.jsonlines\n",
            "Minimizing /content/data/train.chinese.v4_gold_conll\n",
            "Merging clusters (shouldn't happen very often.)\n",
            "Wrote 1810 documents to /content/data/train.chinese.384.jsonlines\n",
            "Minimizing /content/data/test.chinese.v4_gold_conll\n",
            "Wrote 218 documents to /content/data/test.chinese.384.jsonlines\n",
            "Minimizing /content/data/dev.chinese.v4_gold_conll\n",
            "Wrote 252 documents to /content/data/dev.chinese.512.jsonlines\n",
            "Minimizing /content/data/train.chinese.v4_gold_conll\n",
            "Merging clusters (shouldn't happen very often.)\n",
            "Wrote 1810 documents to /content/data/train.chinese.512.jsonlines\n",
            "Minimizing /content/data/test.chinese.v4_gold_conll\n",
            "Wrote 218 documents to /content/data/test.chinese.512.jsonlines\n",
            "max_sent_len_chinese = 512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v-gSDnvhubC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/data/*.jsonlines '/content/drive/My Drive/colab_data/Coreference/span_bert/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_10LZZxobi07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf /content/data/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIMEd074gcjq",
        "colab_type": "code",
        "outputId": "bbd2cd4f-f3cf-4301-ebb4-173933698eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kPlifWaz7M0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/drive/My Drive/colab_data/Coreference/chinese_wwm_L-12_H-768_A-12.zip' ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTaMyxyheWzK",
        "colab_type": "code",
        "outputId": "22253685-7ef4-4e3f-ccf0-2b5002d90309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!unzip '/content/chinese_wwm_L-12_H-768_A-12.zip'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/multi_cased_L-12_H-768_A-12.zip\n",
            "   creating: multi_cased_L-12_H-768_A-12/\n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyp-hCBEgqLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/publish/* /content/data/chinese_L-12_H-768_A-12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTCUZe5ag9th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/data/bert_base/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6IoyMUFgqVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/publish/* /content/data/bert_base"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz6gqxpMhBqr",
        "colab_type": "code",
        "outputId": "bf7ca6f6-9e28-4478-fe84-7fdc72e9bca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/coref"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/coref\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDJv04RMcBOm",
        "colab_type": "text"
      },
      "source": [
        "Запускаем обучение:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsfSLyu5kiXS",
        "colab_type": "code",
        "outputId": "eb6a150d-8cb9-4d21-d2ed-f033c12ec35e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!GPU=0 data_dir=/content/data python ./train.py train_bert_base"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 64.63% on 252 docs\n",
            "Average precision (py): 69.78%\n",
            "Average recall (py): 60.19%\n",
            "I0512 22:21:31.760513 140199826343808 train.py:73] [34000] evaL_f1=0.6463, max_f1=0.6463\n",
            "I0512 22:22:30.854428 140199826343808 train.py:58] [34100] loss=22.42, steps/s=1.48\n",
            "I0512 22:23:26.650951 140199826343808 train.py:58] [34200] loss=14.06, steps/s=1.48\n",
            "I0512 22:24:20.985370 140199826343808 train.py:58] [34300] loss=22.28, steps/s=1.48\n",
            "I0512 22:25:26.151252 140199826343808 train.py:58] [34400] loss=31.15, steps/s=1.48\n",
            "I0512 22:26:22.267604 140199826343808 train.py:58] [34500] loss=21.83, steps/s=1.48\n",
            "I0512 22:27:27.142749 140199826343808 train.py:58] [34600] loss=21.71, steps/s=1.48\n",
            "I0512 22:28:24.843075 140199826343808 train.py:58] [34700] loss=14.16, steps/s=1.48\n",
            "I0512 22:29:22.795822 140199826343808 train.py:58] [34800] loss=17.68, steps/s=1.48\n",
            "I0512 22:30:20.795475 140199826343808 train.py:58] [34900] loss=17.70, steps/s=1.48\n",
            "I0512 22:31:20.923572 140199826343808 train.py:58] [35000] loss=18.59, steps/s=1.48\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 64.79% on 252 docs\n",
            "Average precision (py): 65.53%\n",
            "Average recall (py): 64.07%\n",
            "I0512 22:32:48.053357 140199826343808 train.py:73] [35000] evaL_f1=0.6479, max_f1=0.6479\n",
            "I0512 22:33:43.150464 140199826343808 train.py:58] [35100] loss=16.72, steps/s=1.48\n",
            "I0512 22:34:39.568842 140199826343808 train.py:58] [35200] loss=19.75, steps/s=1.48\n",
            "I0512 22:35:43.000787 140199826343808 train.py:58] [35300] loss=18.91, steps/s=1.48\n",
            "I0512 22:36:41.554715 140199826343808 train.py:58] [35400] loss=20.46, steps/s=1.48\n",
            "I0512 22:37:45.558817 140199826343808 train.py:58] [35500] loss=23.13, steps/s=1.48\n",
            "I0512 22:38:38.831767 140199826343808 train.py:58] [35600] loss=20.31, steps/s=1.48\n",
            "I0512 22:39:34.683916 140199826343808 train.py:58] [35700] loss=15.30, steps/s=1.48\n",
            "I0512 22:40:33.413964 140199826343808 train.py:58] [35800] loss=24.31, steps/s=1.48\n",
            "I0512 22:41:32.569685 140199826343808 train.py:58] [35900] loss=22.65, steps/s=1.48\n",
            "I0512 22:42:32.458529 140199826343808 train.py:58] [36000] loss=24.55, steps/s=1.48\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 64.41% on 252 docs\n",
            "Average precision (py): 73.27%\n",
            "Average recall (py): 57.49%\n",
            "I0512 22:43:54.527595 140199826343808 train.py:73] [36000] evaL_f1=0.6441, max_f1=0.6479\n",
            "I0512 22:44:50.409918 140199826343808 train.py:58] [36100] loss=17.61, steps/s=1.48\n",
            "I0512 22:45:50.092508 140199826343808 train.py:58] [36200] loss=19.71, steps/s=1.48\n",
            "I0512 22:46:46.174920 140199826343808 train.py:58] [36300] loss=16.41, steps/s=1.48\n",
            "I0512 22:47:48.113939 140199826343808 train.py:58] [36400] loss=18.43, steps/s=1.48\n",
            "I0512 22:48:41.674350 140199826343808 train.py:58] [36500] loss=13.58, steps/s=1.48\n",
            "I0512 22:49:45.504256 140199826343808 train.py:58] [36600] loss=23.67, steps/s=1.48\n",
            "I0512 22:50:43.333931 140199826343808 train.py:58] [36700] loss=16.47, steps/s=1.48\n",
            "I0512 22:51:41.884987 140199826343808 train.py:58] [36800] loss=19.81, steps/s=1.48\n",
            "I0512 22:52:41.689595 140199826343808 train.py:58] [36900] loss=18.70, steps/s=1.48\n",
            "I0512 22:53:38.951354 140199826343808 train.py:58] [37000] loss=10.89, steps/s=1.48\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 64.44% on 252 docs\n",
            "Average precision (py): 69.97%\n",
            "Average recall (py): 59.73%\n",
            "I0512 22:54:56.596222 140199826343808 train.py:73] [37000] evaL_f1=0.6444, max_f1=0.6479\n",
            "I0512 22:55:52.052026 140199826343808 train.py:58] [37100] loss=20.92, steps/s=1.48\n",
            "I0512 22:56:53.464990 140199826343808 train.py:58] [37200] loss=22.53, steps/s=1.48\n",
            "I0512 22:57:50.584928 140199826343808 train.py:58] [37300] loss=15.17, steps/s=1.48\n",
            "I0512 22:58:50.302155 140199826343808 train.py:58] [37400] loss=20.97, steps/s=1.48\n",
            "I0512 22:59:45.854647 140199826343808 train.py:58] [37500] loss=17.13, steps/s=1.48\n",
            "I0512 23:00:45.408040 140199826343808 train.py:58] [37600] loss=16.43, steps/s=1.48\n",
            "I0512 23:01:48.893290 140199826343808 train.py:58] [37700] loss=25.62, steps/s=1.48\n",
            "I0512 23:02:47.936899 140199826343808 train.py:58] [37800] loss=17.87, steps/s=1.48\n",
            "I0512 23:03:43.289804 140199826343808 train.py:58] [37900] loss=14.51, steps/s=1.48\n",
            "I0512 23:04:41.996261 140199826343808 train.py:58] [38000] loss=15.95, steps/s=1.48\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 64.63% on 252 docs\n",
            "Average precision (py): 66.73%\n",
            "Average recall (py): 62.69%\n",
            "I0512 23:05:59.419890 140199826343808 train.py:73] [38000] evaL_f1=0.6463, max_f1=0.6479\n",
            "I0512 23:07:00.643174 140199826343808 train.py:58] [38100] loss=18.55, steps/s=1.48\n",
            "I0512 23:07:59.652111 140199826343808 train.py:58] [38200] loss=18.01, steps/s=1.48\n",
            "I0512 23:08:56.528975 140199826343808 train.py:58] [38300] loss=16.78, steps/s=1.48\n",
            "I0512 23:09:49.635278 140199826343808 train.py:58] [38400] loss=17.93, steps/s=1.48\n",
            "I0512 23:10:47.541162 140199826343808 train.py:58] [38500] loss=13.06, steps/s=1.48\n",
            "I0512 23:11:54.131665 140199826343808 train.py:58] [38600] loss=25.25, steps/s=1.48\n",
            "I0512 23:12:52.465051 140199826343808 train.py:58] [38700] loss=15.56, steps/s=1.48\n",
            "I0512 23:13:45.636381 140199826343808 train.py:58] [38800] loss=12.14, steps/s=1.48\n",
            "I0512 23:14:38.465372 140199826343808 train.py:58] [38900] loss=14.58, steps/s=1.48\n",
            "I0512 23:15:41.161614 140199826343808 train.py:58] [39000] loss=17.79, steps/s=1.48\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 64.96% on 252 docs\n",
            "Average precision (py): 68.83%\n",
            "Average recall (py): 61.51%\n",
            "I0512 23:17:11.412604 140199826343808 train.py:73] [39000] evaL_f1=0.6496, max_f1=0.6496\n",
            "I0512 23:18:11.646015 140199826343808 train.py:58] [39100] loss=11.15, steps/s=1.48\n",
            "I0512 23:19:10.732536 140199826343808 train.py:58] [39200] loss=17.19, steps/s=1.48\n",
            "I0512 23:20:11.765889 140199826343808 train.py:58] [39300] loss=18.84, steps/s=1.48\n",
            "I0512 23:21:10.888016 140199826343808 train.py:58] [39400] loss=16.64, steps/s=1.48\n",
            "I0512 23:22:15.521328 140199826343808 train.py:58] [39500] loss=21.52, steps/s=1.48\n",
            "I0512 23:23:08.534415 140199826343808 train.py:58] [39600] loss=15.23, steps/s=1.48\n",
            "I0512 23:24:03.258448 140199826343808 train.py:58] [39700] loss=13.11, steps/s=1.48\n",
            "I0512 23:25:02.270145 140199826343808 train.py:58] [39800] loss=19.15, steps/s=1.48\n",
            "I0512 23:25:52.909525 140199826343808 train.py:58] [39900] loss=8.50, steps/s=1.48\n",
            "I0512 23:26:53.064209 140199826343808 train.py:58] [40000] loss=21.68, steps/s=1.48\n",
            "W0512 23:27:10.167345 140199826343808 saver.py:962] Ignoring: /content/data/train_bert_base/model-35000.meta; No such file or directory\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 64.47% on 252 docs\n",
            "Average precision (py): 67.88%\n",
            "Average recall (py): 61.40%\n",
            "I0512 23:28:20.675619 140199826343808 train.py:73] [40000] evaL_f1=0.6447, max_f1=0.6496\n",
            "I0512 23:29:17.011721 140199826343808 train.py:58] [40100] loss=14.12, steps/s=1.48\n",
            "I0512 23:30:16.798003 140199826343808 train.py:58] [40200] loss=20.08, steps/s=1.48\n",
            "I0512 23:31:17.809447 140199826343808 train.py:58] [40300] loss=18.95, steps/s=1.48\n",
            "I0512 23:32:17.084072 140199826343808 train.py:58] [40400] loss=16.69, steps/s=1.48\n",
            "I0512 23:33:11.267699 140199826343808 train.py:58] [40500] loss=10.75, steps/s=1.48\n",
            "I0512 23:34:16.127025 140199826343808 train.py:58] [40600] loss=24.04, steps/s=1.48\n",
            "I0512 23:35:15.322176 140199826343808 train.py:58] [40700] loss=17.00, steps/s=1.48\n",
            "I0512 23:36:17.541359 140199826343808 train.py:58] [40800] loss=18.43, steps/s=1.48\n",
            "I0512 23:37:15.364151 140199826343808 train.py:58] [40900] loss=15.60, steps/s=1.48\n",
            "I0512 23:38:17.260948 140199826343808 train.py:58] [41000] loss=14.98, steps/s=1.48\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 63.56% on 252 docs\n",
            "Average precision (py): 73.89%\n",
            "Average recall (py): 55.84%\n",
            "I0512 23:39:34.928200 140199826343808 train.py:73] [41000] evaL_f1=0.6356, max_f1=0.6496\n",
            "I0512 23:40:30.261341 140199826343808 train.py:58] [41100] loss=10.26, steps/s=1.48\n",
            "I0512 23:41:25.514958 140199826343808 train.py:58] [41200] loss=16.15, steps/s=1.48\n",
            "I0512 23:42:22.145569 140199826343808 train.py:58] [41300] loss=19.62, steps/s=1.48\n",
            "I0512 23:43:21.578787 140199826343808 train.py:58] [41400] loss=14.05, steps/s=1.48\n",
            "I0512 23:44:17.653241 140199826343808 train.py:58] [41500] loss=11.69, steps/s=1.48\n",
            "I0512 23:45:18.980401 140199826343808 train.py:58] [41600] loss=16.37, steps/s=1.48\n",
            "I0512 23:46:11.264106 140199826343808 train.py:58] [41700] loss=10.57, steps/s=1.48\n",
            "I0512 23:47:08.644894 140199826343808 train.py:58] [41800] loss=10.57, steps/s=1.48\n",
            "I0512 23:48:08.679607 140199826343808 train.py:58] [41900] loss=15.08, steps/s=1.48\n",
            "I0512 23:49:10.562234 140199826343808 train.py:58] [42000] loss=20.62, steps/s=1.48\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.01% on 252 docs\n",
            "Average precision (py): 69.63%\n",
            "Average recall (py): 60.97%\n",
            "I0512 23:50:42.403584 140199826343808 train.py:73] [42000] evaL_f1=0.6501, max_f1=0.6501\n",
            "I0512 23:51:41.675942 140199826343808 train.py:58] [42100] loss=13.05, steps/s=1.48\n",
            "I0512 23:52:34.106587 140199826343808 train.py:58] [42200] loss=13.50, steps/s=1.48\n",
            "I0512 23:53:35.149181 140199826343808 train.py:58] [42300] loss=15.72, steps/s=1.48\n",
            "I0512 23:54:37.505755 140199826343808 train.py:58] [42400] loss=18.72, steps/s=1.48\n",
            "I0512 23:55:34.203534 140199826343808 train.py:58] [42500] loss=15.06, steps/s=1.48\n",
            "I0512 23:56:37.290453 140199826343808 train.py:58] [42600] loss=19.79, steps/s=1.48\n",
            "I0512 23:57:34.490846 140199826343808 train.py:58] [42700] loss=16.39, steps/s=1.48\n",
            "I0512 23:58:33.500394 140199826343808 train.py:58] [42800] loss=13.83, steps/s=1.48\n",
            "I0512 23:59:33.623286 140199826343808 train.py:58] [42900] loss=11.21, steps/s=1.48\n",
            "I0513 00:00:34.207003 140199826343808 train.py:58] [43000] loss=19.33, steps/s=1.48\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.14% on 252 docs\n",
            "Average precision (py): 66.64%\n",
            "Average recall (py): 63.71%\n",
            "I0513 00:02:01.241958 140199826343808 train.py:73] [43000] evaL_f1=0.6514, max_f1=0.6514\n",
            "I0513 00:03:01.196550 140199826343808 train.py:58] [43100] loss=17.77, steps/s=1.48\n",
            "I0513 00:04:00.778351 140199826343808 train.py:58] [43200] loss=17.56, steps/s=1.48\n",
            "I0513 00:04:55.854775 140199826343808 train.py:58] [43300] loss=14.70, steps/s=1.48\n",
            "I0513 00:05:52.171568 140199826343808 train.py:58] [43400] loss=12.12, steps/s=1.48\n",
            "I0513 00:06:49.181438 140199826343808 train.py:58] [43500] loss=14.28, steps/s=1.48\n",
            "I0513 00:07:48.631006 140199826343808 train.py:58] [43600] loss=19.58, steps/s=1.48\n",
            "I0513 00:08:45.551734 140199826343808 train.py:58] [43700] loss=10.51, steps/s=1.48\n",
            "I0513 00:09:46.910547 140199826343808 train.py:58] [43800] loss=11.57, steps/s=1.48\n",
            "I0513 00:10:41.547415 140199826343808 train.py:58] [43900] loss=15.71, steps/s=1.48\n",
            "I0513 00:11:40.551161 140199826343808 train.py:58] [44000] loss=12.71, steps/s=1.48\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.40% on 252 docs\n",
            "Average precision (py): 67.11%\n",
            "Average recall (py): 63.78%\n",
            "I0513 00:13:11.450151 140199826343808 train.py:73] [44000] evaL_f1=0.6540, max_f1=0.6540\n",
            "I0513 00:14:09.319414 140199826343808 train.py:58] [44100] loss=13.86, steps/s=1.48\n",
            "I0513 00:15:10.740250 140199826343808 train.py:58] [44200] loss=13.09, steps/s=1.48\n",
            "I0513 00:16:06.478649 140199826343808 train.py:58] [44300] loss=11.64, steps/s=1.48\n",
            "I0513 00:17:06.766446 140199826343808 train.py:58] [44400] loss=19.86, steps/s=1.48\n",
            "I0513 00:18:02.715209 140199826343808 train.py:58] [44500] loss=15.70, steps/s=1.48\n",
            "I0513 00:19:01.417449 140199826343808 train.py:58] [44600] loss=15.01, steps/s=1.48\n",
            "I0513 00:19:59.650916 140199826343808 train.py:58] [44700] loss=11.80, steps/s=1.48\n",
            "I0513 00:20:54.805049 140199826343808 train.py:58] [44800] loss=11.42, steps/s=1.48\n",
            "I0513 00:21:59.244490 140199826343808 train.py:58] [44900] loss=18.05, steps/s=1.48\n",
            "I0513 00:22:55.420987 140199826343808 train.py:58] [45000] loss=12.96, steps/s=1.48\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.34% on 252 docs\n",
            "Average precision (py): 67.84%\n",
            "Average recall (py): 63.02%\n",
            "I0513 00:24:16.749684 140199826343808 train.py:73] [45000] evaL_f1=0.6534, max_f1=0.6540\n",
            "I0513 00:25:15.523423 140199826343808 train.py:58] [45100] loss=14.49, steps/s=1.48\n",
            "I0513 00:26:15.891482 140199826343808 train.py:58] [45200] loss=11.62, steps/s=1.48\n",
            "I0513 00:27:14.559733 140199826343808 train.py:58] [45300] loss=14.19, steps/s=1.48\n",
            "I0513 00:28:18.603887 140199826343808 train.py:58] [45400] loss=16.17, steps/s=1.48\n",
            "I0513 00:29:14.739482 140199826343808 train.py:58] [45500] loss=11.35, steps/s=1.48\n",
            "I0513 00:30:11.585193 140199826343808 train.py:58] [45600] loss=12.84, steps/s=1.48\n",
            "I0513 00:31:09.762725 140199826343808 train.py:58] [45700] loss=16.35, steps/s=1.48\n",
            "I0513 00:32:10.553465 140199826343808 train.py:58] [45800] loss=16.69, steps/s=1.48\n",
            "I0513 00:33:10.132470 140199826343808 train.py:58] [45900] loss=12.89, steps/s=1.48\n",
            "I0513 00:34:08.736722 140199826343808 train.py:58] [46000] loss=14.08, steps/s=1.48\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.48% on 252 docs\n",
            "Average precision (py): 68.70%\n",
            "Average recall (py): 62.55%\n",
            "I0513 00:35:41.000246 140199826343808 train.py:73] [46000] evaL_f1=0.6548, max_f1=0.6548\n",
            "I0513 00:36:40.508406 140199826343808 train.py:58] [46100] loss=13.36, steps/s=1.48\n",
            "I0513 00:37:39.859357 140199826343808 train.py:58] [46200] loss=11.41, steps/s=1.48\n",
            "I0513 00:38:38.278487 140199826343808 train.py:58] [46300] loss=14.27, steps/s=1.48\n",
            "I0513 00:39:35.593215 140199826343808 train.py:58] [46400] loss=12.75, steps/s=1.48\n",
            "I0513 00:40:33.773741 140199826343808 train.py:58] [46500] loss=9.53, steps/s=1.48\n",
            "I0513 00:41:35.179596 140199826343808 train.py:58] [46600] loss=12.44, steps/s=1.48\n",
            "I0513 00:42:32.129002 140199826343808 train.py:58] [46700] loss=11.72, steps/s=1.48\n",
            "I0513 00:43:28.080008 140199826343808 train.py:58] [46800] loss=13.60, steps/s=1.48\n",
            "I0513 00:44:26.722266 140199826343808 train.py:58] [46900] loss=16.07, steps/s=1.48\n",
            "I0513 00:45:20.048251 140199826343808 train.py:58] [47000] loss=10.28, steps/s=1.48\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.11% on 252 docs\n",
            "Average precision (py): 69.27%\n",
            "Average recall (py): 61.44%\n",
            "I0513 00:46:43.005984 140199826343808 train.py:73] [47000] evaL_f1=0.6511, max_f1=0.6548\n",
            "I0513 00:47:41.992727 140199826343808 train.py:58] [47100] loss=13.01, steps/s=1.48\n",
            "I0513 00:48:40.239886 140199826343808 train.py:58] [47200] loss=12.56, steps/s=1.48\n",
            "I0513 00:49:40.446524 140199826343808 train.py:58] [47300] loss=15.10, steps/s=1.48\n",
            "I0513 00:50:39.328902 140199826343808 train.py:58] [47400] loss=11.77, steps/s=1.48\n",
            "I0513 00:51:44.489439 140199826343808 train.py:58] [47500] loss=12.88, steps/s=1.48\n",
            "I0513 00:52:40.059627 140199826343808 train.py:58] [47600] loss=12.04, steps/s=1.48\n",
            "I0513 00:53:39.358963 140199826343808 train.py:58] [47700] loss=13.56, steps/s=1.48\n",
            "I0513 00:54:33.861250 140199826343808 train.py:58] [47800] loss=10.27, steps/s=1.48\n",
            "I0513 00:55:33.116716 140199826343808 train.py:58] [47900] loss=10.59, steps/s=1.48\n",
            "I0513 00:56:29.449463 140199826343808 train.py:58] [48000] loss=13.49, steps/s=1.48\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.17% on 252 docs\n",
            "Average precision (py): 68.95%\n",
            "Average recall (py): 61.79%\n",
            "I0513 00:57:50.227492 140199826343808 train.py:73] [48000] evaL_f1=0.6517, max_f1=0.6548\n",
            "I0513 00:58:50.173338 140199826343808 train.py:58] [48100] loss=13.79, steps/s=1.48\n",
            "I0513 00:59:48.028191 140199826343808 train.py:58] [48200] loss=11.51, steps/s=1.48\n",
            "I0513 01:00:48.842500 140199826343808 train.py:58] [48300] loss=16.03, steps/s=1.48\n",
            "I0513 01:01:46.639916 140199826343808 train.py:58] [48400] loss=10.52, steps/s=1.48\n",
            "I0513 01:02:43.761217 140199826343808 train.py:58] [48500] loss=9.83, steps/s=1.48\n",
            "I0513 01:03:42.973141 140199826343808 train.py:58] [48600] loss=14.01, steps/s=1.48\n",
            "I0513 01:04:42.540068 140199826343808 train.py:58] [48700] loss=15.35, steps/s=1.48\n",
            "I0513 01:05:39.133709 140199826343808 train.py:58] [48800] loss=10.26, steps/s=1.48\n",
            "I0513 01:06:36.083449 140199826343808 train.py:58] [48900] loss=14.62, steps/s=1.48\n",
            "I0513 01:07:36.427776 140199826343808 train.py:58] [49000] loss=13.92, steps/s=1.49\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.35% on 252 docs\n",
            "Average precision (py): 68.79%\n",
            "Average recall (py): 62.24%\n",
            "I0513 01:08:59.543918 140199826343808 train.py:73] [49000] evaL_f1=0.6535, max_f1=0.6548\n",
            "I0513 01:09:55.702888 140199826343808 train.py:58] [49100] loss=12.21, steps/s=1.48\n",
            "I0513 01:10:55.858789 140199826343808 train.py:58] [49200] loss=12.02, steps/s=1.48\n",
            "I0513 01:11:54.426776 140199826343808 train.py:58] [49300] loss=8.23, steps/s=1.48\n",
            "I0513 01:12:52.785186 140199826343808 train.py:58] [49400] loss=12.43, steps/s=1.48\n",
            "I0513 01:13:49.156406 140199826343808 train.py:58] [49500] loss=10.26, steps/s=1.48\n",
            "I0513 01:14:50.514763 140199826343808 train.py:58] [49600] loss=12.78, steps/s=1.48\n",
            "I0513 01:15:48.510533 140199826343808 train.py:58] [49700] loss=15.47, steps/s=1.48\n",
            "I0513 01:16:42.591085 140199826343808 train.py:58] [49800] loss=8.69, steps/s=1.48\n",
            "I0513 01:17:39.264494 140199826343808 train.py:58] [49900] loss=10.92, steps/s=1.49\n",
            "I0513 01:18:37.402840 140199826343808 train.py:58] [50000] loss=12.55, steps/s=1.49\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.16% on 252 docs\n",
            "Average precision (py): 68.12%\n",
            "Average recall (py): 62.46%\n",
            "I0513 01:19:58.479558 140199826343808 train.py:73] [50000] evaL_f1=0.6516, max_f1=0.6548\n",
            "I0513 01:20:59.418831 140199826343808 train.py:58] [50100] loss=16.56, steps/s=1.48\n",
            "I0513 01:21:58.399310 140199826343808 train.py:58] [50200] loss=10.66, steps/s=1.48\n",
            "I0513 01:22:57.134396 140199826343808 train.py:58] [50300] loss=13.30, steps/s=1.48\n",
            "I0513 01:23:53.590576 140199826343808 train.py:58] [50400] loss=9.98, steps/s=1.48\n",
            "I0513 01:24:53.617798 140199826343808 train.py:58] [50500] loss=12.20, steps/s=1.48\n",
            "I0513 01:25:50.599035 140199826343808 train.py:58] [50600] loss=11.38, steps/s=1.48\n",
            "I0513 01:26:53.537162 140199826343808 train.py:58] [50700] loss=14.39, steps/s=1.48\n",
            "I0513 01:27:53.436182 140199826343808 train.py:58] [50800] loss=10.34, steps/s=1.48\n",
            "I0513 01:28:52.871003 140199826343808 train.py:58] [50900] loss=13.42, steps/s=1.49\n",
            "I0513 01:29:50.117886 140199826343808 train.py:58] [51000] loss=9.75, steps/s=1.49\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.19% on 252 docs\n",
            "Average precision (py): 68.53%\n",
            "Average recall (py): 62.17%\n",
            "I0513 01:31:10.633526 140199826343808 train.py:73] [51000] evaL_f1=0.6519, max_f1=0.6548\n",
            "I0513 01:32:11.265464 140199826343808 train.py:58] [51100] loss=11.68, steps/s=1.48\n",
            "I0513 01:33:07.514407 140199826343808 train.py:58] [51200] loss=10.86, steps/s=1.48\n",
            "I0513 01:34:08.512433 140199826343808 train.py:58] [51300] loss=12.52, steps/s=1.48\n",
            "I0513 01:35:09.432369 140199826343808 train.py:58] [51400] loss=9.22, steps/s=1.48\n",
            "I0513 01:36:09.539566 140199826343808 train.py:58] [51500] loss=11.61, steps/s=1.48\n",
            "I0513 01:36:59.595763 140199826343808 train.py:58] [51600] loss=9.24, steps/s=1.48\n",
            "I0513 01:37:52.565709 140199826343808 train.py:58] [51700] loss=9.49, steps/s=1.49\n",
            "I0513 01:38:42.964715 140199826343808 train.py:58] [51800] loss=7.04, steps/s=1.49\n",
            "I0513 01:39:41.225509 140199826343808 train.py:58] [51900] loss=12.46, steps/s=1.49\n",
            "I0513 01:40:42.847206 140199826343808 train.py:58] [52000] loss=14.56, steps/s=1.49\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.14% on 252 docs\n",
            "Average precision (py): 68.63%\n",
            "Average recall (py): 61.99%\n",
            "I0513 01:42:00.463620 140199826343808 train.py:73] [52000] evaL_f1=0.6514, max_f1=0.6548\n",
            "I0513 01:43:04.708198 140199826343808 train.py:58] [52100] loss=15.24, steps/s=1.48\n",
            "I0513 01:44:09.489520 140199826343808 train.py:58] [52200] loss=17.20, steps/s=1.48\n",
            "I0513 01:45:05.394685 140199826343808 train.py:58] [52300] loss=12.74, steps/s=1.48\n",
            "I0513 01:46:03.822814 140199826343808 train.py:58] [52400] loss=8.53, steps/s=1.48\n",
            "I0513 01:47:03.304343 140199826343808 train.py:58] [52500] loss=14.13, steps/s=1.48\n",
            "I0513 01:47:55.414429 140199826343808 train.py:58] [52600] loss=9.18, steps/s=1.49\n",
            "I0513 01:48:52.812600 140199826343808 train.py:58] [52700] loss=12.95, steps/s=1.49\n",
            "I0513 01:49:51.282149 140199826343808 train.py:58] [52800] loss=10.90, steps/s=1.49\n",
            "I0513 01:50:53.474153 140199826343808 train.py:58] [52900] loss=12.16, steps/s=1.49\n",
            "I0513 01:51:55.122311 140199826343808 train.py:58] [53000] loss=16.42, steps/s=1.49\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.26% on 252 docs\n",
            "Average precision (py): 69.07%\n",
            "Average recall (py): 61.85%\n",
            "I0513 01:53:12.454858 140199826343808 train.py:73] [53000] evaL_f1=0.6526, max_f1=0.6548\n",
            "I0513 01:54:09.065784 140199826343808 train.py:58] [53100] loss=9.77, steps/s=1.48\n",
            "I0513 01:55:06.113673 140199826343808 train.py:58] [53200] loss=9.81, steps/s=1.48\n",
            "I0513 01:56:07.752021 140199826343808 train.py:58] [53300] loss=12.68, steps/s=1.48\n",
            "I0513 01:57:07.093690 140199826343808 train.py:58] [53400] loss=12.70, steps/s=1.48\n",
            "I0513 01:58:10.933017 140199826343808 train.py:58] [53500] loss=10.37, steps/s=1.48\n",
            "I0513 01:59:07.231649 140199826343808 train.py:58] [53600] loss=6.18, steps/s=1.49\n",
            "I0513 02:00:08.681427 140199826343808 train.py:58] [53700] loss=11.55, steps/s=1.49\n",
            "I0513 02:01:09.711408 140199826343808 train.py:58] [53800] loss=16.36, steps/s=1.49\n",
            "I0513 02:02:03.102962 140199826343808 train.py:58] [53900] loss=8.68, steps/s=1.49\n",
            "I0513 02:03:01.015818 140199826343808 train.py:58] [54000] loss=10.24, steps/s=1.49\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.41% on 252 docs\n",
            "Average precision (py): 67.18%\n",
            "Average recall (py): 63.75%\n",
            "I0513 02:04:19.809433 140199826343808 train.py:73] [54000] evaL_f1=0.6541, max_f1=0.6548\n",
            "I0513 02:05:13.507599 140199826343808 train.py:58] [54100] loss=8.21, steps/s=1.48\n",
            "I0513 02:06:09.170240 140199826343808 train.py:58] [54200] loss=11.12, steps/s=1.48\n",
            "I0513 02:07:12.165617 140199826343808 train.py:58] [54300] loss=13.98, steps/s=1.48\n",
            "I0513 02:08:10.508719 140199826343808 train.py:58] [54400] loss=13.61, steps/s=1.49\n",
            "I0513 02:09:08.476650 140199826343808 train.py:58] [54500] loss=11.45, steps/s=1.49\n",
            "I0513 02:10:10.734503 140199826343808 train.py:58] [54600] loss=9.81, steps/s=1.49\n",
            "I0513 02:11:08.134257 140199826343808 train.py:58] [54700] loss=9.64, steps/s=1.49\n",
            "I0513 02:12:10.516957 140199826343808 train.py:58] [54800] loss=15.99, steps/s=1.49\n",
            "I0513 02:13:07.574118 140199826343808 train.py:58] [54900] loss=10.25, steps/s=1.49\n",
            "I0513 02:14:07.932849 140199826343808 train.py:58] [55000] loss=10.29, steps/s=1.49\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.41% on 252 docs\n",
            "Average precision (py): 69.07%\n",
            "Average recall (py): 62.11%\n",
            "I0513 02:15:28.565401 140199826343808 train.py:73] [55000] evaL_f1=0.6541, max_f1=0.6548\n",
            "I0513 02:16:20.566454 140199826343808 train.py:58] [55100] loss=7.95, steps/s=1.48\n",
            "I0513 02:17:13.475667 140199826343808 train.py:58] [55200] loss=11.19, steps/s=1.49\n",
            "I0513 02:18:09.835289 140199826343808 train.py:58] [55300] loss=9.74, steps/s=1.49\n",
            "I0513 02:19:14.750367 140199826343808 train.py:58] [55400] loss=10.34, steps/s=1.49\n",
            "I0513 02:20:12.439851 140199826343808 train.py:58] [55500] loss=9.84, steps/s=1.49\n",
            "I0513 02:21:11.421754 140199826343808 train.py:58] [55600] loss=12.27, steps/s=1.49\n",
            "I0513 02:22:09.763899 140199826343808 train.py:58] [55700] loss=10.26, steps/s=1.49\n",
            "I0513 02:23:06.439108 140199826343808 train.py:58] [55800] loss=9.63, steps/s=1.49\n",
            "I0513 02:24:05.873246 140199826343808 train.py:58] [55900] loss=7.39, steps/s=1.49\n",
            "I0513 02:25:07.130939 140199826343808 train.py:58] [56000] loss=11.85, steps/s=1.49\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.40% on 252 docs\n",
            "Average precision (py): 68.46%\n",
            "Average recall (py): 62.60%\n",
            "I0513 02:26:28.524002 140199826343808 train.py:73] [56000] evaL_f1=0.6540, max_f1=0.6548\n",
            "I0513 02:27:28.268161 140199826343808 train.py:58] [56100] loss=8.26, steps/s=1.48\n",
            "I0513 02:28:32.997813 140199826343808 train.py:58] [56200] loss=9.85, steps/s=1.48\n",
            "I0513 02:29:32.022222 140199826343808 train.py:58] [56300] loss=9.96, steps/s=1.49\n",
            "I0513 02:30:31.576640 140199826343808 train.py:58] [56400] loss=7.75, steps/s=1.49\n",
            "I0513 02:31:26.395547 140199826343808 train.py:58] [56500] loss=8.68, steps/s=1.49\n",
            "I0513 02:32:19.747845 140199826343808 train.py:58] [56600] loss=9.23, steps/s=1.49\n",
            "I0513 02:33:15.820246 140199826343808 train.py:58] [56700] loss=10.56, steps/s=1.49\n",
            "I0513 02:34:15.588876 140199826343808 train.py:58] [56800] loss=14.51, steps/s=1.49\n",
            "I0513 02:35:20.520689 140199826343808 train.py:58] [56900] loss=10.32, steps/s=1.49\n",
            "I0513 02:36:19.197145 140199826343808 train.py:58] [57000] loss=10.47, steps/s=1.49\n",
            "Evaluated 1/252 examples.\n",
            "Evaluated 11/252 examples.\n",
            "Evaluated 21/252 examples.\n",
            "Evaluated 31/252 examples.\n",
            "Evaluated 41/252 examples.\n",
            "Evaluated 51/252 examples.\n",
            "Evaluated 61/252 examples.\n",
            "Evaluated 71/252 examples.\n",
            "Evaluated 81/252 examples.\n",
            "Evaluated 91/252 examples.\n",
            "Evaluated 101/252 examples.\n",
            "Evaluated 111/252 examples.\n",
            "Evaluated 121/252 examples.\n",
            "Evaluated 131/252 examples.\n",
            "Evaluated 141/252 examples.\n",
            "Evaluated 151/252 examples.\n",
            "Evaluated 161/252 examples.\n",
            "Evaluated 171/252 examples.\n",
            "Evaluated 181/252 examples.\n",
            "Evaluated 191/252 examples.\n",
            "Evaluated 201/252 examples.\n",
            "Evaluated 211/252 examples.\n",
            "Evaluated 221/252 examples.\n",
            "Evaluated 231/252 examples.\n",
            "Evaluated 241/252 examples.\n",
            "Evaluated 251/252 examples.\n",
            "Average F1 (py): 65.40% on 252 docs\n",
            "Average precision (py): 68.47%\n",
            "Average recall (py): 62.60%\n",
            "I0513 02:37:36.324560 140199826343808 train.py:73] [57000] evaL_f1=0.6540, max_f1=0.6548\n",
            "2020-05-13 02:37:36.332343: W tensorflow/core/kernels/queue_base.cc:277] _0_padding_fifo_queue: Skipping cancelled enqueue attempt with queue not closed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ0VyealcTRY",
        "colab_type": "text"
      },
      "source": [
        "Лучший результат после 57 эпох обучения:\n",
        "\n",
        "* Лучшая эпоха: 46\n",
        "* Average F1 (py): 65.40%\n",
        "* Average precision (py): 68.47%\n",
        "* Average recall (py): 65.40%\n",
        "\n",
        "\n",
        "Гиперпараметры обучения:\n",
        "\n",
        "`/coref/experiments.conf`\n",
        "\n",
        "```\n",
        "# Computation limits.\n",
        "  max_top_antecedents = 50\n",
        "  max_training_sentences = 5\n",
        "  top_span_ratio = 0.4\n",
        "  max_num_speakers = 20\n",
        "  max_segment_len = 256\n",
        "\n",
        "  # Learning\n",
        "  bert_learning_rate = 1e-5\n",
        "  task_learning_rate = 2e-4\n",
        "  num_docs = 2802\n",
        "\n",
        "  # Model hyperparameters.\n",
        "  dropout_rate = 0.3\n",
        "  ffnn_size = 1000\n",
        "  ffnn_depth = 1\n",
        "  num_epochs = 20\n",
        "  feature_size = 20\n",
        "  max_span_width = 30\n",
        "  use_metadata = true\n",
        "  use_features = true\n",
        "  use_segment_distance = true\n",
        "  model_heads = true\n",
        "  coref_depth = 2\n",
        "  coarse_to_fine = true\n",
        "  fine_grained = true\n",
        "  use_prior = true\n",
        "\n",
        "  adam_eps = 1e-6\n",
        "  task_optimizer = adam\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbzjK3bserhy",
        "colab_type": "text"
      },
      "source": [
        "Логи и обученная модель расположены в папке: `data/train_bert_base`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pV9S8XAcnLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save best result\n",
        "!cp -rf /content/data/train_bert_base/ '/content/drive/My Drive/colab_data/Coreference/span_bert/models/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTS_vCD-6iKZ",
        "colab_type": "code",
        "outputId": "2cb31359-7a9f-43ea-dd09-722b4f2ad701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!ls /content/data/train_bert_base"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\t\t\t\t     model-57000.index\n",
            "events.out.tfevents.1589299071.89c75a8806eb  model-57000.meta\n",
            "model-56000.data-00000-of-00001\t\t     model.max.ckpt.data-00000-of-00001\n",
            "model-56000.index\t\t\t     model.max.ckpt.index\n",
            "model-56000.meta\t\t\t     stdout.log\n",
            "model-57000.data-00000-of-00001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqQniVxx8LD7",
        "colab_type": "code",
        "outputId": "9f482aa0-b7e9-4602-b2fa-1d2f446f79fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 30.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 102kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 112kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 122kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 133kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 194kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (46.1.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLFZWxIe8LsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4TeAZOf8eAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%tensorboard --logdir /content/data/train_bert_base"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYbQ0jgv8jqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}